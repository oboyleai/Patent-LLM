{"cells":[{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/var/folders/mh/2sqd7p2s0331dwc0z2rzgtl40000gn/T/ipykernel_44111/1320163270.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n","  from IPython.core.display import display\n"]},{"ename":"ModuleNotFoundError","evalue":"No module named 'keras'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m display\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_model\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EarlyStopping, ModelCheckpoint\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# from IPython.display import Image\u001b[39;00m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"]}],"source":["from IPython.core.display import display\n","import random\n","from keras.models import load_model\n","from keras.callbacks import EarlyStopping, ModelCheckpoint\n","# from IPython.display import Image\n","from keras.utils import plot_model\n","from keras.optimizers import Adam\n","from keras.layers import LSTM, Dense, Dropout, Embedding, Masking, Bidirectional\n","from keras.models import Sequential, load_model\n","import gc\n","import sklearn\n","import pickle\n","from sklearn.utils import shuffle\n","import re\n","from keras.preprocessing.text import Tokenizer\n","import numpy as np\n","import pandas as pd\n","from IPython.display import HTML"]},{"cell_type":"markdown","metadata":{},"source":["from google.colab import drive"]},{"cell_type":"code","execution_count":64,"metadata":{},"outputs":[],"source":["data = pd.read_csv('./neural_network_patent_query.csv',\n","                   parse_dates=['patent_date'])\n","original_abstracts = list(data['patent_abstract'])"]},{"cell_type":"code","execution_count":65,"metadata":{},"outputs":[],"source":["def format_patent(patent):\n","    \"\"\"Add spaces around punctuation\n","    and remove references to images/citations.\"\"\"\n","\n","    # Add spaces around punctuation\n","    patent = re.sub(r'(?<=[^\\s0-9])(?=[.,;?])', r' ', patent)\n","\n","    # Remove references to figures\n","    patent = re.sub(r'\\((\\d+)\\)', r'', patent)\n","\n","    # Remove double spaces\n","    patent = re.sub(r'\\s\\s', ' ', patent)\n","    return patent"]},{"cell_type":"markdown","metadata":{},"source":["formatted = []"]},{"cell_type":"markdown","metadata":{},"source":["# Iterate through all the original abstracts and apply formatting<br>\n","for a in original_abstracts:<br>\n","    formatted.append(format_patent(a))"]},{"cell_type":"markdown","metadata":{},"source":["# Features and Labels<br>\n","# Note to self: new_texts and new_sequences are the texts and corresponding sequences<br>\n","# from only the abstracts that are 50 or more words (a sequence is vector of ints representing a piece of text)"]},{"cell_type":"markdown","metadata":{},"source":["def make_sequences(texts,<br>\n","                   training_length=50,<br>\n","                   lower=True,<br>\n","                   filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'):<br>\n","#     \n","Turn a set of texts into sequences of integers\n","<br>\n","#     # Create the tokenizer object and train on texts<br>\n","#     tokenizer = Tokenizer(lower=lower, filters=filters)<br>\n","#     tokenizer.fit_on_texts(texts)<br>\n","#     # Create look-up dictionaries and reverse look-ups<br>\n","#     word_idx = tokenizer.word_index<br>\n","#     idx_word = tokenizer.index_word<br>\n","#     num_words = len(word_idx) + 1<br>\n","#     word_counts = tokenizer.word_counts<br>\n","#     print(f'There are {num_words} unique words.')<br>\n","#     # Convert text to sequences of integers<br>\n","#     sequences = tokenizer.texts_to_sequences(texts)<br>\n","#     # Limit to sequences with more than training length tokens<br>\n","#     seq_lengths = [len(x) for x in sequences]<br>\n","#     over_idx = [<br>\n","#         i for i, l in enumerate(seq_lengths) if l > (training_length + 20)<br>\n","#     ]<br>\n","#     new_texts = []<br>\n","#     new_sequences = []<br>\n","#     # Only keep sequences with more than training length tokens<br>\n","#     for i in over_idx:<br>\n","#         new_texts.append(texts[i])<br>\n","#         new_sequences.append(sequences[i])<br>\n","#     training_seq = []<br>\n","#     labels = []<br>\n","#     # Iterate through the sequences of tokens<br>\n","#     for seq in new_sequences:<br>\n","#         # Create multiple training examples from each sequence<br>\n","#         for i in range(training_length, len(seq)):<br>\n","#             # Extract the features and label<br>\n","#             extract = seq[i - training_length:i + 1]<br>\n","#             # Set the features and label<br>\n","#             training_seq.append(extract[:-1])<br>\n","#             labels.append(extract[-1])<br>\n","#     print(f'There are {len(training_seq)} training sequences.')<br>\n","#     # Return everything needed for setting up the model<br>\n","#     return word_idx, idx_word, num_words, word_counts, new_texts, new_sequences, training_seq, labels<br>\n","# TRAINING_LENGTH = 50<br>\n","# filters = '!\"%;[\\\\]^_`{|}~\\t\\n'<br>\n","# word_idx, idx_word, num_words, word_counts, abstracts, sequences, features, labels = make_sequences(<br>\n","#     formatted, TRAINING_LENGTH, lower=False, filters=filters)<br>\n","# # Create training and validation sets. One-hot encoding labels<br>\n","# def create_train_valid(features,<br>\n","#                        labels,<br>\n","#                        num_words,<br>\n","#                        train_fraction=0.7):<br>\n","    \n","Create training and validation features and labels.\n"]},{"cell_type":"markdown","metadata":{},"source":["    # Randomly shuffle features and labels<br>\n","    features, labels = shuffle(features, labels, random_state=50)"]},{"cell_type":"markdown","metadata":{},"source":["    # Decide on number of samples for training<br>\n","    train_end = int(train_fraction * len(labels))"]},{"cell_type":"markdown","metadata":{},"source":["    train_features = np.array(features[:train_end])<br>\n","    valid_features = np.array(features[train_end:])"]},{"cell_type":"markdown","metadata":{},"source":["    train_labels = labels[:train_end]<br>\n","    valid_labels = labels[train_end:]"]},{"cell_type":"markdown","metadata":{},"source":["    # Convert to arrays<br>\n","    X_train, X_valid = np.array(train_features), np.array(valid_features)"]},{"cell_type":"markdown","metadata":{},"source":["    # Using int8 for memory savings<br>\n","    y_train = np.zeros((len(train_labels), num_words), dtype=np.int8)<br>\n","    y_valid = np.zeros((len(valid_labels), num_words), dtype=np.int8)"]},{"cell_type":"markdown","metadata":{},"source":["    # One hot encoding of labels<br>\n","    for example_index, word_index in enumerate(train_labels):<br>\n","        y_train[example_index, word_index] = 1"]},{"cell_type":"markdown","metadata":{},"source":["    for example_index, word_index in enumerate(valid_labels):<br>\n","        y_valid[example_index, word_index] = 1"]},{"cell_type":"markdown","metadata":{},"source":["    # Memory management: one-hot encoding the labels creates massive numpy arrays<br>\n","    # so I took care to delete the un-used objects from the workspace.<br>\n","    import gc<br>\n","    gc.enable()<br>\n","    del features, labels, train_features, valid_features, train_labels, valid_labels<br>\n","    gc.collect()"]},{"cell_type":"markdown","metadata":{},"source":["    return X_train, X_valid, y_train, y_valid"]},{"cell_type":"markdown","metadata":{},"source":["X_train, X_valid, y_train, y_valid = create_train_valid(<br>\n","    features, labels, num_words)"]},{"cell_type":"markdown","metadata":{},"source":["# Save the intermediate results as it is consuming too much memory!!"]},{"cell_type":"markdown","metadata":{},"source":["def save_intermediate_results(datap):<br>\n","    for i in data:<br>\n","        with open(f'./Datasets/{i}.pkl', 'wb') as f:<br>\n","            pickle.dump(globals()[i], f)"]},{"cell_type":"markdown","metadata":{},"source":["data = ['word_idx', 'idx_word', 'num_words', 'word_counts',<br>\n","        'abstracts', 'sequences', 'features', 'labels', 'X_valid', 'y_valid']<br>\n","save_intermediate_results(data)"]},{"cell_type":"markdown","metadata":{},"source":["with open('./Datasets/X_valid.pkl', 'rb') as f:<br>\n","    X_valid = pickle.load(f)<br>\n","    print(X_valid.shape)"]},{"cell_type":"markdown","metadata":{},"source":["# Clean up<br>\n","gc.enable()<br>\n","del (word_idx, idx_word, num_words, word_counts, abstracts,<br>\n","     sequences, features, labels, X_valid, y_valid)<br>\n","gc.collect()"]},{"cell_type":"markdown","metadata":{},"source":["# Build Model: After converting the words into embeddings, we pass them through<br>\n","# a single LSTM layer, then into a fully connected layer with relu activation<br>\n","# before the final output layer with a softmax activation"]},{"cell_type":"markdown","metadata":{},"source":["def make_word_level_model(num_words,<br>\n","                          lstm_cells=64,<br>\n","                          trainable=True,<br>\n","                          lstm_layers=1,<br>\n","                          bi_direc=False):<br>\n","#     \n","Make a word level recurrent neural network with option for pretrained embeddings<br>\n","       and varying numbers of LSTM cell layers.\n"]},{"cell_type":"markdown","metadata":{},"source":["    model = Sequential()"]},{"cell_type":"markdown","metadata":{},"source":["    model.add(<br>\n","        Embedding(<br>\n","            input_dim=num_words,<br>\n","            output_dim=100,<br>\n","            input_length=50,<br>\n","            trainable=True))"]},{"cell_type":"markdown","metadata":{},"source":["    # If want to add multiple LSTM layers<br>\n","    if lstm_layers > 1:<br>\n","        for i in range(lstm_layers - 1):<br>\n","            model.add(<br>\n","                LSTM(<br>\n","                    lstm_cells,<br>\n","                    return_sequences=True,<br>\n","                    dropout=0.1,<br>\n","                    recurrent_dropout=0.1))"]},{"cell_type":"markdown","metadata":{},"source":["    # Add final LSTM cell layer<br>\n","    if bi_direc:<br>\n","        model.add(<br>\n","            Bidirectional(<br>\n","                LSTM(<br>\n","                    lstm_cells,<br>\n","                    return_sequences=False,<br>\n","                    dropout=0.1,<br>\n","                    recurrent_dropout=0.1)))<br>\n","    else:<br>\n","        model.add(<br>\n","            LSTM(<br>\n","                lstm_cells,<br>\n","                return_sequences=False,<br>\n","                dropout=0.1,<br>\n","                recurrent_dropout=0.1))"]},{"cell_type":"markdown","metadata":{},"source":["    model.add(Dense(128, activation='relu'))"]},{"cell_type":"markdown","metadata":{},"source":["    # Dropout for regularization<br>\n","    model.add(Dropout(0.5))"]},{"cell_type":"markdown","metadata":{},"source":["    # Output layer<br>\n","    model.add(Dense(num_words, activation='softmax'))"]},{"cell_type":"markdown","metadata":{},"source":["    # Compile the model<br>\n","    model.compile(<br>\n","        optimizer='adam',<br>\n","        loss='categorical_crossentropy',<br>\n","        metrics=['accuracy'])"]},{"cell_type":"markdown","metadata":{},"source":["    return model"]},{"cell_type":"markdown","metadata":{},"source":["model = make_word_level_model(<br>\n","    num_words=16192,<br>\n","    lstm_cells=64,<br>\n","    trainable=True,<br>\n","    lstm_layers=1)<br>\n","model.summary()"]},{"cell_type":"markdown","metadata":{},"source":["plot_model(model, to_file=f'./Datasets/patent abstract.png', show_shapes=True)"]},{"cell_type":"markdown","metadata":{},"source":["# Train Model!<br>\n","# Early Stopping: Stop training when validation loss no longer decreases<br>\n","# Model Checkpoint: Save the best model on disk"]},{"cell_type":"markdown","metadata":{},"source":["# Patience: Number of epochs with no improvement after which training will be stopped"]},{"cell_type":"markdown","metadata":{},"source":["def make_callbacks(model_name, save=True):<br>\n","#     \n","Make list of callbacks for training\n","<br>\n","#     # callbacks = [EarlyStopping(monitor='val_loss', patience=5)]<br>\n","#     callbacks = []<br>\n","#     if save:<br>\n","#         callbacks.append(<br>\n","#             ModelCheckpoint(<br>\n","#                 f'./Datasets/patent_abstracts.h5',<br>\n","#                 save_best_only=True,<br>\n","#                 save_weights_only=False))<br>\n","#     return callbacks<br>\n","# callbacks = make_callbacks(model)<br>\n","# # Verbose is for showing output<br>\n","# # Batch_size defines the number of samples that will be propagated through the network<br>\n","# # Typically networks train faster with mini-batches. That's because we update the weights after each propagation.<br>\n","# # If we used all samples during propagation we would make only 1 update for the network's parameter.<br>\n","# with open('./Datasets/X_valid.pkl', 'rb') as f:<br>\n","#     X_valid = pickle.load(f)<br>\n","#     print(X_valid.shape)<br>\n","# with open('./Datasets/y_valid.pkl', 'rb') as f:<br>\n","#     y_valid = pickle.load(f)<br>\n","#     print(y_valid.shape)<br>\n","# history = model.fit(<br>\n","#     X_train,<br>\n","#     y_train,<br>\n","#     epochs=5,<br>\n","#     batch_size=128,<br>\n","#     verbose=1,<br>\n","#     callbacks=callbacks,<br>\n","#     validation_data=(X_valid, y_valid))<br>\n","# model.save('patents_abstracts')<br>\n","def load_values(data):<br>\n","    val = []<br>\n","    for i in data:<br>\n","        with open(f'./Datasets/{i}.pkl', 'rb') as f:<br>\n","            globals()[f'{i}'] = pickle.load(f)<br>\n","            print(type(globals()[f'{i}']))<br>\n","            val.append(globals()[f'{i}'])<br>\n","    return val<br>\n","X_valid, y_valid = load_values(['X_valid', 'y_valid'])<br>\n","X_valid.shape<br>\n","model = load_model(\"patents_abstracts\")<br>\n","results = model.evaluate(X_valid, y_valid, batch_size=128)<br>\n","with open('./Datasets/idx_word.pkl', 'rb') as f:<br>\n","    idx_word = pickle.load(f)<br>\n","with open('./Datasets/sequences.pkl', 'rb') as f:<br>\n","    sequences = pickle.load(f)<br>\n","print(len(sequences))<br>\n","print(sequences[0])<br>\n","str_out = \"\"<br>\n","for i in sequences[0]:<br>\n","    str_out += idx_word.get(i) + \" \"<br>\n","def generate_output(model,<br>\n","                    sequences,<br>\n","                    training_length=75,<br>\n","                    new_words=50,<br>\n","                    diversity=1,<br>\n","                    return_output=True,<br>\n","                    n_gen=1):<br>\n","  \n","Generate `new_words` words of output from a trained model and format into HTML.\n"]},{"cell_type":"code","execution_count":66,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'numpy.ndarray'>\n","<class 'numpy.ndarray'>\n","747/747 [==============================] - 16s 21ms/step - loss: 5.2208 - accuracy: 0.2261\n","3255\n","[18, 6149, 3599, 3136, 2003, 12, 2, 426, 683, 1311, 28, 5, 154, 54, 27, 2, 9984, 20, 5, 2976, 102, 9985, 7, 193, 1425, 780, 12, 2, 9986, 1935, 1537, 4, 13, 6149, 3599, 213, 27, 2, 9987, 23, 224, 20, 118, 28, 9988, 2, 5259, 4208, 6, 1425, 7, 2, 1018, 337, 27, 2, 4685, 3600, 3, 4208, 6, 1425, 25, 84, 2, 9989, 225, 3, 3358, 4, 13, 6149, 3599, 2337, 7, 1, 1311, 90, 5, 1289, 283, 90, 1093, 1, 225, 3, 3358, 3, 1, 4685, 472, 5, 6, 203, 2, 519, 1312, 23, 3358, 22, 30, 7, 1, 1311, 25, 1578, 1, 1311, 7, 2, 169, 53, 35, 23, 1217, 7479, 173, 1, 224, 225, 3, 3358, 3, 1, 4685, 472, 4]\n"]}],"source":["def load_values(data):\n","    val = []\n","    for i in data:\n","\n","        with open(f'./Datasets/{i}.pkl', 'rb') as f:\n","            globals()[f'{i}'] = pickle.load(f)\n","            print(type(globals()[f'{i}']))\n","            val.append(globals()[f'{i}'])\n","    return val\n","\n","\n","X_valid, y_valid = load_values(['X_valid', 'y_valid'])\n","X_valid.shape\n","\n","\n","model = load_model(\"patents_abstracts\")\n","results = model.evaluate(X_valid, y_valid, batch_size=128)\n","\n","with open('./Datasets/idx_word.pkl', 'rb') as f:\n","    idx_word = pickle.load(f)\n","\n","with open('./Datasets/sequences.pkl', 'rb') as f:\n","    sequences = pickle.load(f)\n","\n","print(len(sequences))\n","print(sequences[0])\n","str_out = \"\"\n","for i in sequences[0]:\n","    str_out += idx_word.get(i) + \" \"\n","\n","\n","def generate_output(model,\n","                    sequences,\n","                    training_length=75,\n","                    new_words=50,\n","                    diversity=1,\n","                    return_output=True,\n","                    n_gen=1):\n","    \"\"\"Generate `new_words` words of output from a trained model and format into HTML.\"\"\"\n","\n","    # Choose a random sequence\n","    seq = random.choice(sequences)\n","\n","    # Choose a random starting point\n","    seed_idx = random.randint(0, len(seq) - training_length - 10)\n","    # Ending index for seed\n","    end_idx = seed_idx + training_length\n","\n","    gen_list = []\n","\n","    for n in range(n_gen):\n","        # Extract the seed sequence\n","        seed = seq[seed_idx:end_idx]\n","        original_sequence = [idx_word[i] for i in seed]\n","        generated = seed[:] + ['#']\n","\n","        # Find the actual entire sequence\n","        actual = generated[:] + seq[end_idx:end_idx + new_words]\n","\n","        # Keep adding new words\n","        for i in range(new_words):\n","\n","            # Make a prediction from the seed\n","            preds = model.predict(np.array(seed).reshape(1, -1))[0].astype(\n","                np.float64)\n","\n","            # Diversify\n","            preds = np.log(preds) / diversity\n","            exp_preds = np.exp(preds)\n","\n","            # Softmax\n","            preds = exp_preds / sum(exp_preds)\n","\n","            # Choose the next word\n","            probas = np.random.multinomial(1, preds, 1)[0]\n","\n","            next_idx = np.argmax(probas)\n","\n","            # New seed adds on old word\n","            seed = seed[1:] + [next_idx]\n","            generated.append(next_idx)\n","\n","        # Showing generated and actual abstract\n","        n = []\n","\n","        for i in generated:\n","            n.append(idx_word.get(i, ''))\n","\n","        gen_list.append(n)\n","\n","    a = []\n","\n","    for i in actual:\n","        a.append(idx_word.get(i, ''))\n","\n","    a = a[training_length:]\n","\n","    gen_list = [\n","        gen[training_length:training_length + len(a)] for gen in gen_list\n","    ]\n","\n","    if return_output:\n","        return original_sequence, gen_list, a\n","\n","\n","\n","# display stuff\n","\n","\n","def header(text, color='black'):\n","    raw_html = f'<h1 style=\"color: {color};\"><center>' + \\\n","        str(text) + '</center></h1>'\n","    return raw_html\n","\n","\n","def box(text):\n","    raw_html = '<div style=\"border:1px inset black;padding:1em;font-size: 20px;\">' + \\\n","        str(text)+'</div>'\n","    return raw_html\n","\n","\n","def addContent(old_html, raw_html):\n","    old_html += raw_html\n","    return old_html\n","\n","\n","def remove_weird_grammar(str):\n","    return str.replace(' , ', ', ').replace(' . ', '. ')\n","\n","\n","def run_project():\n","    original_sequence, predicted_continuation, actual_continuation = generate_output(\n","    model, sequences, 50)\n","    seed_seq_string = ' '.join(i for i in original_sequence)\n","    seed_seq_string = remove_weird_grammar(seed_seq_string)\n","    pred_seq_string = ' '.join(i for i in predicted_continuation[0])\n","    pred_seq_string = remove_weird_grammar(pred_seq_string)\n","    actual_seq_string = ' '.join(i for i in actual_continuation)\n","    actual_seq_string = remove_weird_grammar(actual_seq_string)\n","\n","    seed_html = ''\n","    seed_html = addContent(seed_html, header(\n","        'Seed Sequence', color='darkblue'))\n","    seed_html = addContent(seed_html,\n","                        box(seed_seq_string))\n","\n","    pred_html = ''\n","    pred_html = addContent(pred_html, header(\n","        'Predicted Sequence', color='darkblue'))\n","    pred_html = addContent(pred_html,\n","                        box(pred_seq_string))\n","\n","    actual_html = ''\n","    actual_html = addContent(actual_html, header(\n","        'Actual Sequence', color='darkblue'))\n","    actual_html = addContent(actual_html,\n","                        box(actual_seq_string))\n","    display(HTML(seed_html))\n","    display(HTML(pred_html))\n","    display(HTML(actual_html))\n"]},{"cell_type":"code","execution_count":67,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 0s 106ms/step\n","1/1 [==============================] - 0s 14ms/step\n","1/1 [==============================] - 0s 12ms/step\n","1/1 [==============================] - 0s 12ms/step\n","1/1 [==============================] - 0s 11ms/step\n","1/1 [==============================] - 0s 13ms/step\n","1/1 [==============================] - 0s 11ms/step\n","1/1 [==============================] - 0s 12ms/step\n","1/1 [==============================] - 0s 12ms/step\n","1/1 [==============================] - 0s 12ms/step\n","1/1 [==============================] - 0s 12ms/step\n","1/1 [==============================] - 0s 12ms/step\n","1/1 [==============================] - 0s 12ms/step\n","1/1 [==============================] - 0s 12ms/step\n","1/1 [==============================] - 0s 12ms/step\n","1/1 [==============================] - 0s 12ms/step\n","1/1 [==============================] - 0s 12ms/step\n","1/1 [==============================] - 0s 12ms/step\n","1/1 [==============================] - 0s 12ms/step\n","1/1 [==============================] - 0s 12ms/step\n","1/1 [==============================] - 0s 13ms/step\n","1/1 [==============================] - 0s 12ms/step\n","1/1 [==============================] - 0s 12ms/step\n","1/1 [==============================] - 0s 12ms/step\n","1/1 [==============================] - 0s 13ms/step\n","1/1 [==============================] - 0s 13ms/step\n","1/1 [==============================] - 0s 12ms/step\n","1/1 [==============================] - 0s 13ms/step\n","1/1 [==============================] - 0s 13ms/step\n","1/1 [==============================] - 0s 12ms/step\n","1/1 [==============================] - 0s 12ms/step\n","1/1 [==============================] - 0s 12ms/step\n","1/1 [==============================] - 0s 12ms/step\n","1/1 [==============================] - 0s 12ms/step\n","1/1 [==============================] - 0s 12ms/step\n","1/1 [==============================] - 0s 12ms/step\n","1/1 [==============================] - 0s 12ms/step\n","1/1 [==============================] - 0s 13ms/step\n","1/1 [==============================] - 0s 11ms/step\n","1/1 [==============================] - 0s 12ms/step\n","1/1 [==============================] - 0s 12ms/step\n","1/1 [==============================] - 0s 11ms/step\n","1/1 [==============================] - 0s 11ms/step\n","1/1 [==============================] - 0s 12ms/step\n","1/1 [==============================] - 0s 12ms/step\n","1/1 [==============================] - 0s 12ms/step\n","1/1 [==============================] - 0s 11ms/step\n","1/1 [==============================] - 0s 12ms/step\n","1/1 [==============================] - 0s 12ms/step\n","1/1 [==============================] - 0s 12ms/step\n"]},{"data":{"text/html":["<h1 style=\"color: darkblue;\"><center>Seed Sequence</center></h1><div style=\"border:1px inset black;padding:1em;font-size: 20px;\">information obtaining device which non-invasively and iteratively obtains information relating to propagation of a pulse wave through an arterial vessel of the subject, a second information obtaining device which non-invasively and iteratively obtains information relating to heartbeat of the subject and/or information relating to area of a heartbeat-synchronous pulse</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<h1 style=\"color: darkblue;\"><center>Predicted Sequence</center></h1><div style=\"border:1px inset black;padding:1em;font-size: 20px;\"> address. In an color vectors is then compared to selected output detecting the samples. In another embodiment, activation of search and/or connections network .sub end voltages, the first insulator is A synapses comprising one and image does used to produce with plurality of phonemes along the</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<h1 style=\"color: darkblue;\"><center>Actual Sequence</center></h1><div style=\"border:1px inset black;padding:1em;font-size: 20px;\"> of a volumetric pulse wave obtained from a peripheral body portion of the subject, and an estimating device including a neural network which learns sets of information each set of which includes a blood-pressure value measured using a cuff, pulse-wave-propagation-relating information obtained when the blood-pressure value is measured</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["run_project()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"}},"nbformat":4,"nbformat_minor":2}
